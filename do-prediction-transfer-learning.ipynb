{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dataset placeholder - adapt to your files\n",
    "# -----------------------------\n",
    "class MyTimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects:\n",
    "      X: np.array (N, seq_len, n_features)\n",
    "      y: np.array (N,)  # scalar DO target value\n",
    "    Replace the `from_arrays` usage with file loading/parsing.\n",
    "    \"\"\"\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        assert X.ndim == 3\n",
    "        assert len(X) == len(y)\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_arrays(X, y):\n",
    "        return MyTimeSeriesDataset(np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utility metrics (paper metrics)\n",
    "# -----------------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    # avoid division by zero\n",
    "    denom = np.where(np.abs(y_true) < 1e-8, 1e-8, y_true)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot != 0 else 0.0\n",
    "\n",
    "def index_of_agreement(y_true, y_pred):\n",
    "    num = np.sum((y_true - y_pred)**2)\n",
    "    denom = np.sum((np.abs(y_true - np.mean(y_true)) + np.abs(y_pred - np.mean(y_true)))**2)\n",
    "    return float(1 - num / denom) if denom != 0 else 0.0\n",
    "\n",
    "def nash_sutcliffe(y_true, y_pred):\n",
    "    num = np.sum((y_pred - y_true)**2)\n",
    "    denom = np.sum((y_true - np.mean(y_true))**2)\n",
    "    return float(1 - num / denom) if denom != 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Model blocks\n",
    "# -----------------------------\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_sizes, stride=1):\n",
    "        super().__init__()\n",
    "        # kernel_sizes: list/tuple e.g. [1,3,1]\n",
    "        ks1, ks2, ks3 = kernel_sizes\n",
    "        # Use padding to keep control of length\n",
    "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=ks1, stride=stride, padding=ks1//2)\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=ks2, stride=1, padding=ks2//2)\n",
    "        self.conv3 = nn.Conv1d(out_ch, out_ch, kernel_size=ks3, stride=1, padding=ks3//2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln = nn.LayerNorm(out_ch)\n",
    "\n",
    "        # if in_ch != out_ch or stride !=1, we make a projection\n",
    "        if in_ch != out_ch or stride != 1:\n",
    "            self.shortcut = nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.shortcut = None\n",
    "\n",
    "    def forward(self, x):  # x: [batch, channels, seq_len]\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)  # [batch, out_ch, seq_len']\n",
    "        # layernorm across channel dim: LayerNorm expects last dim features -> transpose\n",
    "        out_t = out.transpose(1, 2)  # [batch, seq_len, channels]\n",
    "        out_t = self.ln(out_t)\n",
    "        out = out_t.transpose(1, 2)\n",
    "        if self.shortcut is not None:\n",
    "            sc = self.shortcut(x)\n",
    "        else:\n",
    "            sc = x\n",
    "        return self.relu(out + sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Fusion model: ResNet branch, BiLSTM branch, Attention branch, concat -> FC\n",
    "# -----------------------------\n",
    "class DOTransferModel(nn.Module):\n",
    "    def __init__(self, n_features, seq_len, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # We'll implement three ResNet blocks as parallel branch: but paper fed same input to 3 branches\n",
    "        # Branch 1: ResNet Block 1 -> out filters 32\n",
    "        self.res1 = ResBlock1D(in_ch=n_features, out_ch=32, kernel_sizes=(1,3,1), stride=1)\n",
    "        self.res2 = ResBlock1D(in_ch=32, out_ch=64, kernel_sizes=(1,5,1), stride=2)  # stride 2 as paper\n",
    "        self.res3 = ResBlock1D(in_ch=64, out_ch=128, kernel_sizes=(1,3,1), stride=1)\n",
    "\n",
    "        # After conv, do temporal global average pooling -> vector\n",
    "        # BiLSTM branch: three stacked BiLSTM layers configured as in paper: units 16,32,64\n",
    "        self.bilstm1 = nn.LSTM(input_size=n_features, hidden_size=16, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.bilstm2 = nn.LSTM(input_size=32, hidden_size=32, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.bilstm3 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        # We'll pass input sequentially through lstm1->lstm2->lstm3 by projecting features\n",
    "\n",
    "        # Attention branch: we'll use three multihead attention modules (different embed dims & heads)\n",
    "        # MultiheadAttention in PyTorch expects (seq_len, batch, embed_dim) if batch_first=False\n",
    "        self.attn_proj1 = nn.Linear(n_features, 16)\n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=16, num_heads=4, batch_first=True)\n",
    "        self.attn_proj2 = nn.Linear(n_features, 32)\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "        self.attn_proj3 = nn.Linear(n_features, 64)\n",
    "        self.mha3 = nn.MultiheadAttention(embed_dim=64, num_heads=8, batch_first=True)\n",
    "\n",
    "        # Combine sizes:\n",
    "        # ResNet final filters = 128, we'll global pool over time -> 128\n",
    "        # BiLSTM final hidden (bidirectional) -> 64*2 = 128 (we'll take last hidden)\n",
    "        # Attention branches -> produce pooled feature sizes: 16 + 32 + 64 = 112\n",
    "        combined = 128 + 128 + (16 + 32 + 64)\n",
    "        # Fully connected projection head from paper: two FC layers 128, 128 then 1\n",
    "        self.fc1 = nn.Linear(combined, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc_out = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):  # x: [batch, seq_len, n_features]\n",
    "        b, seq_len, nfeat = x.shape\n",
    "\n",
    "        # ---- ResNet branch ----\n",
    "        # conv1d expects (batch, channels, seq_len)\n",
    "        xr = x.transpose(1, 2)  # [b, n_features, seq_len]\n",
    "        xr = self.res1(xr)\n",
    "        xr = self.res2(xr)\n",
    "        xr = self.res3(xr)  # [b, 128, seq_len']\n",
    "        # global average pooling across time dimension\n",
    "        xr_pool = xr.mean(dim=2)  # [b, 128]\n",
    "\n",
    "        # ---- BiLSTM branch ----\n",
    "        xb = x  # [b, seq_len, n_features]\n",
    "        out1, _ = self.bilstm1(xb)    # out1: [b, seq_len, 32]\n",
    "        out2, _ = self.bilstm2(out1)  # out2: [b, seq_len, 64]\n",
    "        out3, (hn, cn) = self.bilstm3(out2)  # out3: [b, seq_len, 128]\n",
    "        # hn has shape (num_layers*2, b, hidden_size) => take last layer both directions\n",
    "        # We'll flatten last layer states\n",
    "        # hn[-2] is last forward, hn[-1] is last backward (since num_layers=1)\n",
    "        if hn.shape[0] >= 2:\n",
    "            h_fwd = hn[-2]\n",
    "            h_bwd = hn[-1]\n",
    "            hb = torch.cat([h_fwd, h_bwd], dim=1)  # [b, 128]\n",
    "        else:\n",
    "            hb = hn[-1]\n",
    "        # hb is [b, 128]\n",
    "\n",
    "        # ---- Attention branch ----\n",
    "        # proj -> MHA\n",
    "        xa1 = self.attn_proj1(x)  # [b, seq_len, 16]\n",
    "        attn_out1, _ = self.mha1(xa1, xa1, xa1)  # [b, seq_len, 16]\n",
    "        a1 = attn_out1.mean(dim=1)  # pool -> [b, 16]\n",
    "\n",
    "        xa2 = self.attn_proj2(x)\n",
    "        attn_out2, _ = self.mha2(xa2, xa2, xa2)\n",
    "        a2 = attn_out2.mean(dim=1)  # [b, 32]\n",
    "\n",
    "        xa3 = self.attn_proj3(x)\n",
    "        attn_out3, _ = self.mha3(xa3, xa3, xa3)\n",
    "        a3 = attn_out3.mean(dim=1)  # [b, 64]\n",
    "\n",
    "        attn_concat = torch.cat([a1, a2, a3], dim=1)  # [b, 112]\n",
    "\n",
    "        # ---- concat and fc ----\n",
    "        cat = torch.cat([xr_pool, hb, attn_concat], dim=1)\n",
    "        h = self.relu(self.fc1(cat))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.fc2(h))\n",
    "        out = self.fc_out(h)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training & evaluation helpers\n",
    "# -----------------------------\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    tot_loss = 0.0\n",
    "    for xb, yb in dataloader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(xb)\n",
    "        loss = criterion(yhat, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot_loss += loss.item() * xb.size(0)\n",
    "    return tot_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for xb, yb in dataloader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        yhat = model(xb)\n",
    "        preds.append(yhat.detach().cpu().numpy())\n",
    "        trues.append(yb.detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    return trues, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level train function\n",
    "def fit(model, train_dataset, val_dataset=None, epochs=50, batch_size=64, lr=1e-3, device='cpu'):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) if val_dataset is not None else None\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_state = None\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        if val_loader is not None:\n",
    "            y_val, y_pred = evaluate(model, val_loader, device)\n",
    "            val_mse = np.mean((y_val - y_pred)**2)\n",
    "            if val_mse < best_val_loss:\n",
    "                best_val_loss = val_mse\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "        # optional: print progress\n",
    "        if ep % 10 == 0 or ep == 1 or ep==epochs:\n",
    "            print(f\"[Epoch {ep}/{epochs}] train_loss={loss:.6f} val_mse={best_val_loss:.6f}\")\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_finetune(pretrained_path: str, target_train_dataset, target_val_dataset,\n",
    "                      lr=1e-4, epochs=30, batch_size=64, device='cpu', freeze_base=True):\n",
    "    # Rebuild same model architecture\n",
    "    sample = target_train_dataset[0][0]\n",
    "    n_features = sample.shape[1]\n",
    "    seq_len    = sample.shape[0]\n",
    "    model = DOTransferModel(n_features=n_features, seq_len=seq_len, device=device)\n",
    "\n",
    "    # Load pretrained weights\n",
    "    state_dict = torch.load(pretrained_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Freeze base layers if requested\n",
    "    if freeze_base:\n",
    "        for name, p in model.named_parameters():\n",
    "            if any(k in name for k in ['fc1', 'fc2', 'fc_out']):\n",
    "                p.requires_grad = True\n",
    "            else:\n",
    "                p.requires_grad = False\n",
    "\n",
    "    # Re-init FC layers\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "    model.fc1.apply(weights_init)\n",
    "    model.fc2.apply(weights_init)\n",
    "    model.fc_out.apply(weights_init)\n",
    "\n",
    "    # Train only unfrozen parameters\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(params, lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(target_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(target_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    best_state, best_val = None, float('inf')\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(xb)\n",
    "            loss = criterion(yhat, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # Validation\n",
    "        y_val, y_pred = evaluate(model, val_loader, device)\n",
    "        val_mse = np.mean((y_val - y_pred)**2)\n",
    "        if val_mse < best_val:\n",
    "            best_val, best_state = val_mse, model.state_dict().copy()\n",
    "\n",
    "        if ep % 5 == 0:\n",
    "            print(f\"[FT Epoch {ep}/{epochs}] train_loss={tot_loss/len(train_loader.dataset):.6f} val_mse={val_mse:.6f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# K-fold evaluation for small target dataset (paper used K-fold on Lake Y)\n",
    "# -----------------------------\n",
    "def k_fold_evaluate(model_constructor, X, y, k=5, epochs=40, device='cpu'):\n",
    "    \"\"\"\n",
    "    model_constructor: a function that returns an uninitialized model instance.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    metrics_list = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"---- Fold {fold+1}/{k} ----\")\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        y_tr, y_te = y[train_idx], y[test_idx]\n",
    "        ds_tr = MyTimeSeriesDataset.from_arrays(X_tr, y_tr)\n",
    "        ds_te = MyTimeSeriesDataset.from_arrays(X_te, y_te)\n",
    "        model = model_constructor()\n",
    "        model = fit(model, ds_tr, val_dataset=ds_te, epochs=epochs, batch_size=64, lr=1e-3, device=device)\n",
    "        trues, preds = evaluate(model, DataLoader(ds_te, batch_size=64), device)\n",
    "        trues = trues.flatten(); preds = preds.flatten()\n",
    "        metrics = {\n",
    "            'RMSE': rmse(trues, preds),\n",
    "            'MAPE': mape(trues, preds),\n",
    "            'R2': r2_score_np(trues, preds),\n",
    "            'd': index_of_agreement(trues, preds),\n",
    "            'NSE': nash_sutcliffe(trues, preds)\n",
    "        }\n",
    "        print(metrics)\n",
    "        metrics_list.append(metrics)\n",
    "    # Average metrics\n",
    "    avg = {k: np.mean([m[k] for m in metrics_list]) for k in metrics_list[0]}\n",
    "    print(\"Average metrics across folds:\", avg)\n",
    "    return metrics_list, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('River water quality data; DO.xlsx', sheet_name='USGS 14162500')\n",
    "df1 = df1.drop(columns=['Time '])\n",
    "\n",
    "df2 = pd.read_excel('River water quality data; DO.xlsx', sheet_name='USGS 14211010')\n",
    "df2 = df2.drop(columns=['Time '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df1.drop('DO', axis=1))\n",
    "\n",
    "# Apply to DataFrame\n",
    "scaled_array1 = scaler.transform(df1.drop(\"DO\", axis=1))\n",
    "scaled_array2 = scaler.transform(df2.drop(\"DO\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2557, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_array1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.3],\n",
       "       [10.4],\n",
       "       [10.3],\n",
       "       ...,\n",
       "       [10.8],\n",
       "       [10.8],\n",
       "       [10.8]], shape=(2557, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['DO'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put back into DataFrame with same column names & index\n",
    "df1 = pd.DataFrame(np.concatenate((scaled_array1, df1['DO'].values.reshape(-1, 1)), axis=1),\n",
    "                   columns=df1.columns, index=df1.index)\n",
    "\n",
    "# Put back into DataFrame with same column names & index\n",
    "df2 = pd.DataFrame(np.concatenate((scaled_array2, df2['DO'].values.reshape(-1, 1)), axis=1),\n",
    "                   columns=df2.columns, index=df2.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Sequence builder for both lakes\n",
    "# -----------------------------\n",
    "def make_sequences(df, seq_len=60, pred_horizon=60):\n",
    "    features = df[['WT','Q','SC','PH','Tu']].values\n",
    "    target   = df['DO'].values\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len - pred_horizon):\n",
    "        X.append(features[i:i+seq_len])\n",
    "        y.append(target[i+seq_len+pred_horizon-1])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2557, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2557, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_train, df1_test = df1[:2200], df1[2200:]\n",
    "df2_train, df2_test = df2[:2200], df2[2200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, pred_horizon = 60, 60\n",
    "\n",
    "X1, y1 = make_sequences(df1_train, seq_len, pred_horizon)   # Lake T\n",
    "X2, y2 = make_sequences(df2_train, seq_len, pred_horizon)   # Lake Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test, y1_test = make_sequences(df1_test, seq_len, pred_horizon) \n",
    "X2_test, y2_test = make_sequences(df2_test, seq_len, pred_horizon) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining on Lake T...\n",
      "[Epoch 1/50] train_loss=72.621436 val_mse=inf\n",
      "[Epoch 10/50] train_loss=0.676872 val_mse=inf\n",
      "[Epoch 20/50] train_loss=0.464859 val_mse=inf\n",
      "[Epoch 30/50] train_loss=0.413393 val_mse=inf\n",
      "[Epoch 40/50] train_loss=0.387647 val_mse=inf\n",
      "[Epoch 50/50] train_loss=0.395033 val_mse=inf\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 2. Train on Lake T (pretraining)\n",
    "# -----------------------------\n",
    "train_ds1 = MyTimeSeriesDataset.from_arrays(X1, y1)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "def make_model(): return DOTransferModel(n_features=X1.shape[2], seq_len=seq_len, device=device)\n",
    "\n",
    "print(\"Pretraining on Lake T...\")\n",
    "pre_model = make_model()\n",
    "pre_model = fit(pre_model, train_ds1, val_dataset=None,\n",
    "                epochs=50, batch_size=128, lr=1e-3, device=device)\n",
    "\n",
    "torch.save(pre_model.state_dict(), \"lakeT_pretrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning (transfer learning) on Lake Y...\n",
      "[FT Epoch 5/60] train_loss=0.987344 val_mse=0.438083\n",
      "[FT Epoch 10/60] train_loss=0.984240 val_mse=0.345338\n",
      "[FT Epoch 15/60] train_loss=1.012057 val_mse=0.395659\n",
      "[FT Epoch 20/60] train_loss=0.895623 val_mse=0.448899\n",
      "[FT Epoch 25/60] train_loss=0.871433 val_mse=0.374025\n",
      "[FT Epoch 30/60] train_loss=0.842582 val_mse=0.518583\n",
      "[FT Epoch 35/60] train_loss=0.808310 val_mse=0.451760\n",
      "[FT Epoch 40/60] train_loss=0.695780 val_mse=0.585027\n",
      "[FT Epoch 45/60] train_loss=0.647125 val_mse=0.536099\n",
      "[FT Epoch 50/60] train_loss=0.600480 val_mse=1.372382\n",
      "[FT Epoch 55/60] train_loss=0.490912 val_mse=1.548131\n",
      "[FT Epoch 60/60] train_loss=0.488483 val_mse=2.459424\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Lake Y splits\n",
    "# -----------------------------\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y2, test_size=0.2, shuffle=False)\n",
    "train_ds2 = MyTimeSeriesDataset.from_arrays(X2_train, y2_train)\n",
    "val_ds2   = MyTimeSeriesDataset.from_arrays(X2_val,   y2_val)\n",
    "\n",
    "# -----------------------------\n",
    "# 4A. Transfer learning on Lake Y\n",
    "# -----------------------------\n",
    "print(\"\\nFine-tuning (transfer learning) on Lake Y...\")\n",
    "ft_model = transfer_finetune(\"lakeT_pretrained.pth\",\n",
    "                             target_train_dataset=train_ds2,\n",
    "                             target_val_dataset=val_ds2,\n",
    "                             lr=5e-4, epochs=60, batch_size=32,\n",
    "                             device=device, freeze_base=True)\n",
    "\n",
    "y_true_ft, y_pred_ft = evaluate(ft_model, DataLoader(val_ds2, batch_size=64), device)\n",
    "metrics_ft = {\n",
    "    \"RMSE\": rmse(y_true_ft, y_pred_ft),\n",
    "    \"MAPE\": mape(y_true_ft, y_pred_ft),\n",
    "    \"R2\":   r2_score_np(y_true_ft, y_pred_ft),\n",
    "    \"d\":    index_of_agreement(y_true_ft, y_pred_ft),\n",
    "    \"NSE\":  nash_sutcliffe(y_true_ft, y_pred_ft)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training from scratch on Lake Y...\n",
      "[Epoch 1/50] train_loss=23.508194 val_mse=1.382350\n",
      "[Epoch 10/50] train_loss=0.575694 val_mse=0.177256\n",
      "[Epoch 20/50] train_loss=0.461997 val_mse=0.177256\n",
      "[Epoch 30/50] train_loss=0.361116 val_mse=0.177256\n",
      "[Epoch 40/50] train_loss=0.235467 val_mse=0.177256\n",
      "[Epoch 50/50] train_loss=0.126602 val_mse=0.177256\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4B. Training from scratch on Lake Y\n",
    "# -----------------------------\n",
    "print(\"\\nTraining from scratch on Lake Y...\")\n",
    "scratch_model = make_model()\n",
    "scratch_model = fit(scratch_model, train_ds2, val_dataset=val_ds2,\n",
    "                    epochs=50, batch_size=32, lr=1e-3, device=device)\n",
    "\n",
    "y_true_sc, y_pred_sc = evaluate(scratch_model, DataLoader(val_ds2, batch_size=64), device)\n",
    "metrics_sc = {\n",
    "    \"RMSE\": rmse(y_true_sc, y_pred_sc),\n",
    "    \"MAPE\": mape(y_true_sc, y_pred_sc),\n",
    "    \"R2\":   r2_score_np(y_true_sc, y_pred_sc),\n",
    "    \"d\":    index_of_agreement(y_true_sc, y_pred_sc),\n",
    "    \"NSE\":  nash_sutcliffe(y_true_sc, y_pred_sc)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results on Lake Y ===\n",
      "Transfer learning: {'RMSE': 1.5682551860809326, 'MAPE': 12.741239547729492, 'R2': -0.18608629703521729, 'd': 0.7412799596786499, 'NSE': -0.18608629703521729}\n",
      "From scratch:      {'RMSE': 0.4210173487663269, 'MAPE': 2.9985365867614746, 'R2': 0.9145163893699646, 'd': 0.9761101007461548, 'NSE': 0.9145163893699646}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. Compare results\n",
    "# -----------------------------\n",
    "print(\"\\n=== Results on Lake Y ===\")\n",
    "print(\"Transfer learning:\", metrics_ft)\n",
    "print(\"From scratch:     \", metrics_sc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
